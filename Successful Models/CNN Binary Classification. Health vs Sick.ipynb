{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports all necessary libraries for building, training, evaluating, and visualizing a convolutional neural network for image classification\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Rescaling, Flatten\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3JBgGOsKu44",
    "outputId": "75970c95-0a6d-40b5-a64d-a4465059c4b3"
   },
   "outputs": [],
   "source": [
    "# Displays an example image from the dataset to determine the input size for the neural network\n",
    "%matplotlib inline\n",
    "pil_im = Image.open('/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/training/Healthy/26 (100).jpg', 'r')\n",
    "imshow(np.asarray(pil_im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25NfZptPxjgD",
    "outputId": "403c2d08-18c7-4265-99ff-5231a879acb0"
   },
   "outputs": [],
   "source": [
    "# Defines data source and parameters\n",
    "image_size = (100, 100)  # Defines images of 100x100 dimension\n",
    "batch_size = 32          # Defines number of images that are passed to the network per epoch\n",
    "\n",
    "# Loads training dataset from directory\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/training/\",\n",
    "    validation_split = 0.2,     # Uses 20% of images for validation\n",
    "    subset = \"training\",\n",
    "    seed = 1337,                # Sets random seed for reproducibility\n",
    "    image_size = image_size,    # Sets size of dataset images\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'  # Uses categorical labels for multi-class classification\n",
    ")\n",
    "\n",
    "# Loads test dataset from directory\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/test\",\n",
    "    validation_split = 0.2,     # Uses 20% of images for validation\n",
    "    subset = \"validation\",\n",
    "    seed = 1337,                # Sets random seed for reproducibility\n",
    "    image_size = image_size,    # Sets size of dataset images\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'\n",
    ")\n",
    "\n",
    "# Loads validation dataset from directory\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/validation\",\n",
    "    validation_split = 0.2,     # Uses 20% of images for validation\n",
    "    subset = \"validation\",\n",
    "    seed = 1337,                # Sets random seed for reproducibility\n",
    "    image_size = image_size,    # Sets size of dataset images\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'\n",
    ")\n",
    "\n",
    "# Prefetches data for performance optimization\n",
    "train_ds = train_ds.prefetch(buffer_size = 32)\n",
    "test_ds = test_ds.prefetch(buffer_size = 32)\n",
    "val_ds = val_ds.prefetch(buffer_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of images in each class in the training dataset\n",
    "! ls -1 '/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/training/Healthy' | wc -l\n",
    "! ls -1 '/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/training/Sick' | wc -l\n",
    "\n",
    "# Training directory path\n",
    "train_dir = \"/kaggle/input/alzheimer-mri-healthy-vs-sick/Processed Dataset/training\"\n",
    "\n",
    "# Counting the number of images in each class\n",
    "class_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in os.listdir(train_dir)}\n",
    "\n",
    "# Converting to DataFrame to be used for visualization\n",
    "df = pd.DataFrame(list(class_counts.items()), columns=[\"Class\", \"Count\"])\n",
    "\n",
    "# Plotting the number of images per class\n",
    "plt.figure(figsize = (15, 8))\n",
    "ax = sns.barplot(x = df[\"Class\"], y = df[\"Count\"], palette = \"Set1\")\n",
    "ax.set_xlabel(\"Class\", fontsize = 20)\n",
    "ax.set_ylabel(\"Count\", fontsize = 20)\n",
    "plt.title(\"The Number Of Samples For Each Class\", fontsize = 20)\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5tVrY9gxyfJ"
   },
   "outputs": [],
   "source": [
    "# Initializes the model in Sequential mode, adding one layer after another\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Defines the architecture of the convolutional neural network, including normalization, convolutional, pooling, dropout, flatten, and dense layers for binary classification\n",
    "model.add(Rescaling(scale = (1. / 127.5),  # Adds a layer for normalization of the dataset image set\n",
    "                    offset = -1,\n",
    "                    input_shape = (100, 100, 3)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the model by specifying the loss function, optimizer, and evaluation metric to prepare for training\n",
    "model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays a summary and a visual diagram of the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Saves and displays the model architecture\n",
    "tf.keras.utils.plot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True, show_dtype = True, dpi = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AiE5BwazFFK",
    "outputId": "aa303034-9054-4789-a7e2-ad7462f9dce3"
   },
   "outputs": [],
   "source": [
    "# Trains the model using the training and validation datasets, applying early stopping to prevent overfitting\n",
    "epochs = 200  # Sets the number of epochs for training\n",
    "\n",
    "# Defines early stopping to prevent overfitting\n",
    "# Stops training if validation accuracy does not improve for 10 epochs\n",
    "es = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 10, restore_best_weights = True)\n",
    "\n",
    "# Fits the model to the training and validation data\n",
    "h = model.fit(\n",
    "        train_ds,\n",
    "        epochs = epochs,\n",
    "        validation_data = val_ds,\n",
    "        callbacks = [es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PPKm5euoRAs",
    "outputId": "75322679-5efa-4867-84c0-029df58ea4a0"
   },
   "outputs": [],
   "source": [
    "# Graphical representation of the results obtained during the training phase\n",
    "# Plots the training and validation accuracy and loss over epochs to visualize the model's learning progress\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'])\n",
    "plt.plot(h.history['loss'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation','loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkyaTtnAKfqA",
    "outputId": "bf82d8a9-7714-446f-ac4b-920bee3a5f1c"
   },
   "outputs": [],
   "source": [
    "# Obtains predictions and true labels\n",
    "results = np.concatenate([(y, model.predict(x = x)) for x, y in val_ds], axis = 1)\n",
    "\n",
    "# Gets predicted class indices\n",
    "predictions = np.argmax(results[0], axis = 1)\n",
    "\n",
    "# Gets true class indices\n",
    "labels = np.argmax(results[1], axis = 1)\n",
    "\n",
    "# Calculates confusion matrix\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plots confusion matrix\n",
    "sns.heatmap(cf_matrix, annot = True, fmt = \".2f\", cmap = \"Blues\")\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(labels, predictions, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines class names for Alzheimer's MRI dataset\n",
    "class_names = ['Healthy', 'Sick']\n",
    "\n",
    "# Generates predictions for the test dataset\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "# Converts predictions to class labels\n",
    "y_pred = np.argmax(predictions, axis = 1)\n",
    "\n",
    "# Retrieves true labels from the test dataset\n",
    "y_real = np.concatenate([y for x, y in test_ds], axis = 0)\n",
    "y_real = np.argmax(y_real, axis = 1)  # Converts one-hot labels to class indices\n",
    "\n",
    "# Computes confusion matrix\n",
    "cm = confusion_matrix(y_real, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis] * 100  # Converts to percentage format\n",
    "\n",
    "# Plots confusion matrix as a heatmap\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.heatmap(cm_percent, annot = True, fmt = \".2f\", cmap = \"Blues\", xticklabels = class_names, yticklabels = class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the predicted probabilities, predicted class, and actual class for each test sample, indicating whether each prediction is correct\n",
    "for p, l in zip(predictions, y_real):\n",
    "    probs_percent = [f\"{prob*100:.2f}%\" for prob in p]  # Convert probabilities to percentage\n",
    "    predicted_class_idx = np.argmax(p)  # Gets predicted class index\n",
    "    predicted_class_name = class_names[predicted_class_idx]  # Gets predicted class name\n",
    "\n",
    "    print(f\"Predictions: {probs_percent} -> Predicted class: {predicted_class_name} (Class {predicted_class_idx}), Actual Label: {class_names[l]}\")\n",
    "\n",
    "    if predicted_class_idx == l:\n",
    "        print(\"Correct ✅\\n\")\n",
    "    else:\n",
    "        print(\"Incorrect ❌\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN. Natural Images Recogniser.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6517250,
     "sourceId": 10531472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
