{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries for data processing, visualization, and neural network construction\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Rescaling, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3JBgGOsKu44",
    "outputId": "75970c95-0a6d-40b5-a64d-a4465059c4b3"
   },
   "outputs": [],
   "source": [
    "# Displays an example image to determine input size for the network\n",
    "%matplotlib inline\n",
    "pil_im = Image.open('/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/training/Non Demented/26 (100).jpg', 'r')\n",
    "imshow(np.asarray(pil_im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25NfZptPxjgD",
    "outputId": "403c2d08-18c7-4265-99ff-5231a879acb0"
   },
   "outputs": [],
   "source": [
    "# Loads and prepares training, validation, and test datasets with batching and prefetching\n",
    "image_size = (100, 100)  # Defines image dimensions\n",
    "batch_size = 32          # Sets batch size for training\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/training\",\n",
    "    validation_split = 0.2,     # Splits 20% of images for validation\n",
    "    subset = \"training\",\n",
    "    seed = 1337,                # Sets random seed for reproducibility\n",
    "    image_size = image_size,    # Sets image size\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'  # Uses categorical labels for multi-class classification\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/validation\",\n",
    "    validation_split = 0.2,     # Splits 20% of images for validation\n",
    "    subset = \"validation\",\n",
    "    seed = 1337,                # Sets random seed for reproducibility\n",
    "    image_size = image_size,    # Sets image size\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/test\",\n",
    "    validation_split = 0.2,     # Splits 20% of images for validation\n",
    "    subset = \"validation\",\n",
    "    seed = 1337,                # Sets random seed for reproducibility\n",
    "    image_size = image_size,    # Sets image size\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size = 32)  # Prefetches training data\n",
    "val_ds = val_ds.prefetch(buffer_size = 32)      # Prefetches validation data\n",
    "test_ds = test_ds.prefetch(buffer_size = 32)    # Prefetches test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts and visualizes the number of images per class in the training set\n",
    "! ls -1 '/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/training/Non Demented' | wc -l\n",
    "! ls -1 '/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/training/Very Mild Demented' | wc -l\n",
    "\n",
    "# Defines training directory path\n",
    "train_dir = \"/kaggle/input/alzheimers-dataset-early-stage-vs-health/Processed Dataset/training\"\n",
    "\n",
    "# Counts the number of images in each class\n",
    "class_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in os.listdir(train_dir)}\n",
    "\n",
    "# Converts class counts to DataFrame\n",
    "df = pd.DataFrame(list(class_counts.items()), columns = [\"Class\", \"Count\"])\n",
    "\n",
    "# Plots the number of images per class\n",
    "plt.figure(figsize = (15, 8))\n",
    "ax = sns.barplot(x = df[\"Class\"], y = df[\"Count\"], palette = \"Set1\")\n",
    "ax.set_xlabel(\"Class\", fontsize = 20)\n",
    "ax.set_ylabel(\"Count\", fontsize = 20)\n",
    "plt.title(\"The Number Of Samples For Each Class\", fontsize = 20)\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds and compiles the convolutional neural network model\n",
    "model = keras.Sequential()  # Initializes sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5tVrY9gxyfJ"
   },
   "outputs": [],
   "source": [
    "model.add(Rescaling(scale = (1. / 127.5),  # Adds normalization layer\n",
    "                    offset = -1,\n",
    "                    input_shape = (100, 100, 3)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiles the model with loss, optimizer, and accuracy metric\n",
    "model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer = tf.keras.optimizers.Adam(1e-3),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Summarizes and visualizes the model architecture\n",
    "model.summary()\n",
    "\n",
    "# Saves a visual representation of the model to a file\n",
    "tf.keras.utils.plot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True, show_dtype = True, dpi = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AiE5BwazFFK",
    "outputId": "aa303034-9054-4789-a7e2-ad7462f9dce3"
   },
   "outputs": [],
   "source": [
    "# Trains the model with early stopping based on validation accuracy\n",
    "epochs = 200  # Sets number of training epochs\n",
    "\n",
    "# Configures early stopping callback\n",
    "es = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 10, restore_best_weights = True)\n",
    "\n",
    "# Fits the model to training and validation data\n",
    "h = model.fit(\n",
    "        train_ds,\n",
    "        epochs = epochs,\n",
    "        validation_data = val_ds,\n",
    "        callbacks = [es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PPKm5euoRAs",
    "outputId": "75322679-5efa-4867-84c0-029df58ea4a0"
   },
   "outputs": [],
   "source": [
    "# Plots training and validation accuracy and loss over epochs, side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize = (14, 5))\n",
    "\n",
    "# Plots training and validation accuracy\n",
    "axs[0].plot(h.history['accuracy'], label = 'Training Accuracy')\n",
    "axs[0].plot(h.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(loc = 'lower right')\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plots training and validation loss\n",
    "axs[1].plot(h.history['loss'], label = 'Training Loss')\n",
    "axs[1].plot(h.history['val_loss'], label = 'Validation Loss')\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(loc = 'upper right')\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkyaTtnAKfqA",
    "outputId": "bf82d8a9-7714-446f-ac4b-920bee3a5f1c"
   },
   "outputs": [],
   "source": [
    "# Computes and visualizes the confusion matrix for validation data with class names\n",
    "class_names = ['Healthy', 'Sick']\n",
    "\n",
    "# Concatenates true and predicted labels\n",
    "results = np.concatenate([(y, model.predict(x = x)) for x, y in val_ds], axis = 1)\n",
    "\n",
    "# Gets predicted class indices\n",
    "predictions = np.argmax(results[0], axis = 1)\n",
    "\n",
    "# Gets true class indices\n",
    "labels = np.argmax(results[1], axis = 1)\n",
    "\n",
    "# Computes confusion matrix\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plots confusion matrix\n",
    "sns.heatmap(cf_matrix, annot = True, fmt = \"d\", cmap = \"Blues\", xticklabels = class_names, yticklabels = class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (Validation Set)\")\n",
    "plt.show()\n",
    "\n",
    "# Prints classification report\n",
    "print(classification_report(labels, predictions, target_names = class_names, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates predictions for the test set and obtain the true labels\n",
    "y_pred = np.argmax(model.predict(test_ds), axis = 1)\n",
    "y_real = np.concatenate([y for x, y in test_ds], axis = 0)\n",
    "y_real = np.argmax(y_real, axis = 1)\n",
    "\n",
    "# Plots the confusion matrix as percentages for the test set\n",
    "cm = confusion_matrix(y_real, y_pred)  # Computes confusion matrix for test set\n",
    "cm_percent = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis] * 100  # Converts confusion matrix to percentage format\n",
    "\n",
    "# Plots confusion matrix as heatmap\n",
    "plt.figure(figsize = (6, 6))\n",
    "sns.heatmap(cm_percent, annot = True, fmt = \".2f\", cmap = \"Blues\", xticklabels = class_names, yticklabels = class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes predictions on the test set and compares them to true labels\n",
    "class_names = ['Healthy', 'Sick']\n",
    "\n",
    "# Generates predictions for the test dataset\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "# Converts predictions to class labels\n",
    "y_pred = np.argmax(predictions, axis = 1)\n",
    "\n",
    "# Retrieves true labels from test dataset\n",
    "y_real = np.concatenate([y for x, y in test_ds], axis = 0)\n",
    "y_real = np.argmax(y_real, axis = 1)  # Converts one-hot labels to class indices\n",
    "\n",
    "# Displays predictions and compares with true labels\n",
    "for p, l in zip(predictions, y_real):\n",
    "    probs_percent = [f\"{prob*100:.2f}%\" for prob in p]       # Converts probabilities to percentage\n",
    "    predicted_class_idx = np.argmax(p)                       # Gets predicted class index\n",
    "    predicted_class_name = class_names[predicted_class_idx]  # Gets predicted class name\n",
    "\n",
    "    print(f\"Predictions: {probs_percent} -> Predicted class: {predicted_class_name} (Class {predicted_class_idx}), Actual Label: {class_names[l]}\")\n",
    "\n",
    "    if predicted_class_idx == l:\n",
    "        print(\"Correct ✅\\n\")\n",
    "    else:\n",
    "        print(\"Incorrect ❌\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN. Natural Images Recogniser.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6742066,
     "sourceId": 10854594,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
