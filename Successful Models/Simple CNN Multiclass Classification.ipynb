{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports all necessary libraries for building, training, evaluating, and visualizing a convolutional neural network for multiclass image classification\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Rescaling, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3JBgGOsKu44",
    "outputId": "75970c95-0a6d-40b5-a64d-a4465059c4b3"
   },
   "outputs": [],
   "source": [
    "# Displays an example image from the dataset to determine the input size for the neural network\n",
    "%matplotlib inline\n",
    "pil_im = Image.open('/kaggle/input/alzheimers-mri-images/Processed Dataset/training/Non Demented/26 (100).jpg', 'r')\n",
    "imshow(np.asarray(pil_im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25NfZptPxjgD",
    "outputId": "403c2d08-18c7-4265-99ff-5231a879acb0"
   },
   "outputs": [],
   "source": [
    "# Loads the training, validation, and test datasets from directories, sets image size and batch size, and optimizes data loading with prefetching\n",
    "image_size = (100, 100)  # Images of 100x100 dimension\n",
    "batch_size = 32          # Number of images that are passed to the network per epoch\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimers-mri-images/Processed Dataset/training\",\n",
    "    validation_split = 0.2,     # 20% of images for validation\n",
    "    subset = \"training\",\n",
    "    seed = 1337,                # In an iteration, we do not feed the network with all the images but with small portions given by the batch_size\n",
    "    image_size = image_size,    # Size of our dataset images\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'  # The network classifies more than two classes (categorical), if the network would classify two classes (binary)\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimers-mri-images/Processed Dataset/validation\",\n",
    "    validation_split = 0.2,     # 80% of images for training\n",
    "    subset = \"validation\",\n",
    "    seed = 1337,                # This is a specific way of randomizing the data used during the training phase of the network\n",
    "    image_size = image_size,    # Recommended images sizes (100, 200, 512)\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/kaggle/input/alzheimers-mri-images/Processed Dataset/test\",\n",
    "    validation_split = 0.2,     # 80% of images for training\n",
    "    subset = \"training\",\n",
    "    seed = 1337,                # This is a specific way of randomizing the data used during the training phase of the network\n",
    "    image_size = image_size,    # Recommended images sizes (100, 200, 512)\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical'\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size = 32)\n",
    "val_ds = val_ds.prefetch(buffer_size = 32)\n",
    "test_ds = test_ds.prefetch(buffer_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates and visualizes the number of images in each class within the training dataset\n",
    "train_dir = \"/kaggle/input/alzheimers-mri-images/Processed Dataset/training\"\n",
    "\n",
    "# Counts the number of images in each class\n",
    "class_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in os.listdir(train_dir)}\n",
    "\n",
    "# Converts to DataFrame\n",
    "df = pd.DataFrame(list(class_counts.items()), columns=[\"Class\", \"Count\"])\n",
    "\n",
    "# Plots the number of images per class\n",
    "plt.figure(figsize = (15, 8))\n",
    "ax = sns.barplot(x = df[\"Class\"], y = df[\"Count\"], palette = \"Set1\")\n",
    "ax.set_xlabel(\"Class\", fontsize = 20)\n",
    "ax.set_ylabel(\"Count\", fontsize = 20)\n",
    "plt.title(\"The Number Of Samples For Each Class\", fontsize = 20)\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5tVrY9gxyfJ"
   },
   "outputs": [],
   "source": [
    "# Initializes a sequential model for building the CNN architecture\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Builds the convolutional neural network model\n",
    "model.add(Rescaling(scale = (1. / 127.5),  # Layer for the normalization of the dataset image set\n",
    "                    offset = -1,\n",
    "                    input_shape = (100, 100, 3)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size = (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.7))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies the model compilation mode\n",
    "model.compile(loss = tf.keras.losses.categorical_crossentropy,\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "# Displays a summary and a visual diagram of the model architecture\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True, show_dtype = True, dpi = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AiE5BwazFFK",
    "outputId": "aa303034-9054-4789-a7e2-ad7462f9dce3"
   },
   "outputs": [],
   "source": [
    "# Trains the model using the training and validation datasets, applying early stopping to prevent overfitting\n",
    "epochs = 200\n",
    "\n",
    "# Once the highest value is reached in the network and it stabilizes or starts\n",
    "# to have losses, the training process is stopped. The parameter patience=10 makes\n",
    "# it stop after 10 waiting epochs when the highest value has been recorded and the\n",
    "# model has not improved, so that finally we will have the best value and therefore\n",
    "# the best weights\n",
    "es = EarlyStopping(monitor = 'val_accuracy', mode = 'max', verbose = 1, patience = 10, restore_best_weights = True)\n",
    "\n",
    "# Fits the model given training and validation data, the number of epochs and each of the previously implemented layers\n",
    "h = model.fit(\n",
    "        train_ds,\n",
    "        epochs = epochs,\n",
    "        validation_data = val_ds,\n",
    "        callbacks = [es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PPKm5euoRAs",
    "outputId": "75322679-5efa-4867-84c0-029df58ea4a0"
   },
   "outputs": [],
   "source": [
    "# Graphical representation of the results obtained during the training phase\n",
    "plt.plot(h.history['accuracy'])\n",
    "plt.plot(h.history['val_accuracy'])\n",
    "plt.plot(h.history['loss'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation','loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkyaTtnAKfqA",
    "outputId": "bf82d8a9-7714-446f-ac4b-920bee3a5f1c"
   },
   "outputs": [],
   "source": [
    "# Concatenates true labels and model predictions for all batches in the validation set\n",
    "results = np.concatenate([(y, model.predict(x = x)) for x, y in val_ds], axis = 1)\n",
    "\n",
    "# Gets predicted class indices from model outputs\n",
    "predictions = np.argmax(results[0], axis = 1)\n",
    "\n",
    "# Gets true class indices from one-hot encoded labels\n",
    "labels = np.argmax(results[1], axis = 1)\n",
    "\n",
    "# Computes the confusion matrix\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plots the confusion matrix as a heatmap\n",
    "sns.heatmap(cf_matrix, annot = True, fmt = \"d\", cmap = \"Blues\")\n",
    "\n",
    "# Prints a detailed classification report\n",
    "print(classification_report(labels, predictions, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djAlHGLz-Cjj",
    "outputId": "f5cda494-9d6f-4b95-843c-862d68696bdb"
   },
   "outputs": [],
   "source": [
    "# Concatenates true labels and model predictions for all batches in the validation set\n",
    "results = np.concatenate([(y, model.predict(x = x)) for x, y in val_ds], axis = 1)\n",
    "\n",
    "# Gets predicted class indices from model outputs\n",
    "predictions = np.argmax(results[0], axis = 1)\n",
    "\n",
    "# Gets true class indices from one-hot encoded labels\n",
    "labels = np.argmax(results[1], axis = 1)\n",
    "\n",
    "# Computes the confusion matrix\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plots the confusion matrix as a heatmap\n",
    "sns.heatmap(cf_matrix, annot = True, fmt = \"d\", cmap = \"Blues\")\n",
    "\n",
    "# Prints a detailed classification report\n",
    "print(classification_report(labels, predictions, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenates true labels and model predictions for all batches in the validation set\n",
    "results = np.concatenate([(y, model.predict(x = x)) for x, y in val_ds], axis = 1)\n",
    "\n",
    "# Gets predicted class indices from model outputs\n",
    "predictions = np.argmax(results[0], axis = 1)\n",
    "\n",
    "# Gets true class indices from one-hot encoded labels\n",
    "labels = np.argmax(results[1], axis = 1)\n",
    "\n",
    "# Computes the confusion matrix\n",
    "cf_matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plots the confusion matrix as a heatmap\n",
    "sns.heatmap(cf_matrix, annot = True, fmt = \"d\", cmap = \"Blues\")\n",
    "\n",
    "# Prints a detailed classification report\n",
    "print(classification_report(labels, predictions, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines class names for Alzheimer's MRI dataset\n",
    "class_names = ['Mild Demented', 'Moderate Demented', \"Non Demented\", \"Very Mild Demented\"]\n",
    "\n",
    "# Generates predictions for the test dataset\n",
    "predictions = model.predict(test_ds)\n",
    "\n",
    "y_pred = np.argmax(predictions, axis = 1)\n",
    "y_real = np.concatenate([y for x, y in test_ds], axis = 0)\n",
    "y_real = np.argmax(y_real, axis = 1)  # Converts one-hot labels to class indices\n",
    "\n",
    "for p, l in zip(predictions, y_real):\n",
    "    probs_percent = [f\"{prob*100:.2f}%\" for prob in p]  # Converts probabilities to percentage\n",
    "    predicted_class_idx = np.argmax(p)  # Index of the predicted class\n",
    "    predicted_class_name = class_names[predicted_class_idx]  # Name of the predicted class\n",
    "\n",
    "    print(f\"Predictions: {probs_percent} -> Predicted class: {predicted_class_name} (Class {predicted_class_idx}), Actual Label: {class_names[l]}\")\n",
    "\n",
    "    if predicted_class_idx == l:\n",
    "        print(\"Correct ✅\\n\")\n",
    "    else:\n",
    "        print(\"Incorrect ❌\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN. Natural Images Recogniser.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6409027,
     "sourceId": 10349832,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
